{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64619fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = \"data/TestVideo/UAV and hands.mp4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "30dde78b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting keras_cv\n",
      "  Downloading keras_cv-0.9.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\lenovo\\appdata\\roaming\\python\\python313\\site-packages (from keras_cv) (25.0)\n",
      "Requirement already satisfied: absl-py in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from keras_cv) (2.3.1)\n",
      "Requirement already satisfied: regex in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from keras_cv) (2024.11.6)\n",
      "Collecting tensorflow-datasets (from keras_cv)\n",
      "  Downloading tensorflow_datasets-4.9.9-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting keras-core (from keras_cv)\n",
      "  Downloading keras_core-0.1.7-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting kagglehub (from keras_cv)\n",
      "  Downloading kagglehub-0.4.0-py3-none-any.whl.metadata (38 kB)\n",
      "Collecting kagglesdk<1.0,>=0.1.14 (from kagglehub->keras_cv)\n",
      "  Downloading kagglesdk-0.1.14-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from kagglehub->keras_cv) (6.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from kagglehub->keras_cv) (2.32.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from kagglehub->keras_cv) (4.67.1)\n",
      "Requirement already satisfied: protobuf in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from kagglesdk<1.0,>=0.1.14->kagglehub->keras_cv) (5.29.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from keras-core->keras_cv) (2.1.3)\n",
      "Requirement already satisfied: rich in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from keras-core->keras_cv) (13.9.4)\n",
      "Requirement already satisfied: namex in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from keras-core->keras_cv) (0.1.0)\n",
      "Requirement already satisfied: h5py in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from keras-core->keras_cv) (3.12.1)\n",
      "Collecting dm-tree (from keras-core->keras_cv)\n",
      "  Downloading dm_tree-0.1.9-cp313-cp313-win_amd64.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: attrs>=18.2.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from dm-tree->keras-core->keras_cv) (24.3.0)\n",
      "Requirement already satisfied: wrapt>=1.11.2 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from dm-tree->keras-core->keras_cv) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests->kagglehub->keras_cv) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests->kagglehub->keras_cv) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests->kagglehub->keras_cv) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests->kagglehub->keras_cv) (2025.4.26)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from rich->keras-core->keras_cv) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\lenovo\\appdata\\roaming\\python\\python313\\site-packages (from rich->keras-core->keras_cv) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras-core->keras_cv) (0.1.0)\n",
      "Collecting etils>=1.9.1 (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets->keras_cv)\n",
      "  Downloading etils-1.13.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting immutabledict (from tensorflow-datasets->keras_cv)\n",
      "  Downloading immutabledict-4.2.2-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting promise (from tensorflow-datasets->keras_cv)\n",
      "  Downloading promise-2.3.tar.gz (19 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: psutil in c:\\users\\lenovo\\appdata\\roaming\\python\\python313\\site-packages (from tensorflow-datasets->keras_cv) (7.0.0)\n",
      "Requirement already satisfied: pyarrow in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from tensorflow-datasets->keras_cv) (19.0.0)\n",
      "Collecting simple_parsing (from tensorflow-datasets->keras_cv)\n",
      "  Downloading simple_parsing-0.1.7-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting tensorflow-metadata (from tensorflow-datasets->keras_cv)\n",
      "  Downloading tensorflow_metadata-1.17.3-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: termcolor in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from tensorflow-datasets->keras_cv) (3.3.0)\n",
      "Requirement already satisfied: toml in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from tensorflow-datasets->keras_cv) (0.10.2)\n",
      "Collecting einops (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets->keras_cv)\n",
      "  Downloading einops-0.8.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: fsspec in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets->keras_cv) (2025.3.2)\n",
      "Collecting importlib_resources (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets->keras_cv)\n",
      "  Downloading importlib_resources-6.5.2-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: typing_extensions in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets->keras_cv) (4.12.2)\n",
      "Requirement already satisfied: zipp in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets->keras_cv) (3.21.0)\n",
      "Requirement already satisfied: six in c:\\users\\lenovo\\appdata\\roaming\\python\\python313\\site-packages (from promise->tensorflow-datasets->keras_cv) (1.17.0)\n",
      "Collecting docstring-parser<1.0,>=0.15 (from simple_parsing->tensorflow-datasets->keras_cv)\n",
      "  Downloading docstring_parser-0.17.0-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting googleapis-common-protos<2,>=1.56.4 (from tensorflow-metadata->tensorflow-datasets->keras_cv)\n",
      "  Downloading googleapis_common_protos-1.72.0-py3-none-any.whl.metadata (9.4 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\lenovo\\appdata\\roaming\\python\\python313\\site-packages (from tqdm->kagglehub->keras_cv) (0.4.6)\n",
      "Downloading keras_cv-0.9.0-py3-none-any.whl (650 kB)\n",
      "   ---------------------------------------- 0.0/650.7 kB ? eta -:--:--\n",
      "   --------------------------------------- 650.7/650.7 kB 20.0 MB/s eta 0:00:00\n",
      "Downloading kagglehub-0.4.0-py3-none-any.whl (69 kB)\n",
      "Downloading kagglesdk-0.1.14-py3-none-any.whl (159 kB)\n",
      "Downloading keras_core-0.1.7-py3-none-any.whl (950 kB)\n",
      "   ---------------------------------------- 0.0/950.8 kB ? eta -:--:--\n",
      "   --------------------------------------- 950.8/950.8 kB 35.7 MB/s eta 0:00:00\n",
      "Downloading dm_tree-0.1.9-cp313-cp313-win_amd64.whl (102 kB)\n",
      "Downloading tensorflow_datasets-4.9.9-py3-none-any.whl (5.3 MB)\n",
      "   ---------------------------------------- 0.0/5.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 5.3/5.3 MB 55.0 MB/s eta 0:00:00\n",
      "Downloading etils-1.13.0-py3-none-any.whl (170 kB)\n",
      "Downloading einops-0.8.1-py3-none-any.whl (64 kB)\n",
      "Downloading immutabledict-4.2.2-py3-none-any.whl (4.7 kB)\n",
      "Downloading importlib_resources-6.5.2-py3-none-any.whl (37 kB)\n",
      "Downloading simple_parsing-0.1.7-py3-none-any.whl (112 kB)\n",
      "Downloading docstring_parser-0.17.0-py3-none-any.whl (36 kB)\n",
      "Downloading tensorflow_metadata-1.17.3-py3-none-any.whl (31 kB)\n",
      "Downloading googleapis_common_protos-1.72.0-py3-none-any.whl (297 kB)\n",
      "Building wheels for collected packages: promise\n",
      "  Building wheel for promise (setup.py): started\n",
      "  Building wheel for promise (setup.py): finished with status 'done'\n",
      "  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21545 sha256=40e962f15914c7a38970a950a01351f7102fa1ce7eb4655f9cf78f63c6747882\n",
      "  Stored in directory: C:\\Users\\Lenovo\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-3x_1rvm1\\wheels\\8f\\46\\1c\\1f4e5d73a20eb816ead5014e97cdeb3928cf314fc46c7bab61\n",
      "Successfully built promise\n",
      "Installing collected packages: promise, importlib_resources, immutabledict, googleapis-common-protos, etils, einops, docstring-parser, dm-tree, tensorflow-metadata, simple_parsing, kagglesdk, keras-core, kagglehub, tensorflow-datasets, keras_cv\n",
      "\n",
      "   ----- ----------------------------------  2/15 [immutabledict]\n",
      "   -------- -------------------------------  3/15 [googleapis-common-protos]\n",
      "   ---------- -----------------------------  4/15 [etils]\n",
      "   ---------- -----------------------------  4/15 [etils]\n",
      "   ------------- --------------------------  5/15 [einops]\n",
      "   ---------------- -----------------------  6/15 [docstring-parser]\n",
      "   ------------------------ ---------------  9/15 [simple_parsing]\n",
      "   -------------------------- ------------- 10/15 [kagglesdk]\n",
      "   -------------------------- ------------- 10/15 [kagglesdk]\n",
      "   -------------------------- ------------- 10/15 [kagglesdk]\n",
      "   ----------------------------- ---------- 11/15 [keras-core]\n",
      "   ----------------------------- ---------- 11/15 [keras-core]\n",
      "   ----------------------------- ---------- 11/15 [keras-core]\n",
      "   ----------------------------- ---------- 11/15 [keras-core]\n",
      "   ----------------------------- ---------- 11/15 [keras-core]\n",
      "   ----------------------------- ---------- 11/15 [keras-core]\n",
      "   ----------------------------- ---------- 11/15 [keras-core]\n",
      "   ----------------------------- ---------- 11/15 [keras-core]\n",
      "   ----------------------------- ---------- 11/15 [keras-core]\n",
      "   ----------------------------- ---------- 11/15 [keras-core]\n",
      "   ----------------------------- ---------- 11/15 [keras-core]\n",
      "   ----------------------------- ---------- 11/15 [keras-core]\n",
      "   -------------------------------- ------- 12/15 [kagglehub]\n",
      "   ---------------------------------- ----- 13/15 [tensorflow-datasets]\n",
      "   ---------------------------------- ----- 13/15 [tensorflow-datasets]\n",
      "   ---------------------------------- ----- 13/15 [tensorflow-datasets]\n",
      "   ---------------------------------- ----- 13/15 [tensorflow-datasets]\n",
      "   ---------------------------------- ----- 13/15 [tensorflow-datasets]\n",
      "   ---------------------------------- ----- 13/15 [tensorflow-datasets]\n",
      "   ---------------------------------- ----- 13/15 [tensorflow-datasets]\n",
      "   ---------------------------------- ----- 13/15 [tensorflow-datasets]\n",
      "   ---------------------------------- ----- 13/15 [tensorflow-datasets]\n",
      "   ---------------------------------- ----- 13/15 [tensorflow-datasets]\n",
      "   ---------------------------------- ----- 13/15 [tensorflow-datasets]\n",
      "   ---------------------------------- ----- 13/15 [tensorflow-datasets]\n",
      "   ---------------------------------- ----- 13/15 [tensorflow-datasets]\n",
      "   ---------------------------------- ----- 13/15 [tensorflow-datasets]\n",
      "   ---------------------------------- ----- 13/15 [tensorflow-datasets]\n",
      "   ---------------------------------- ----- 13/15 [tensorflow-datasets]\n",
      "   ---------------------------------- ----- 13/15 [tensorflow-datasets]\n",
      "   ---------------------------------- ----- 13/15 [tensorflow-datasets]\n",
      "   ---------------------------------- ----- 13/15 [tensorflow-datasets]\n",
      "   ---------------------------------- ----- 13/15 [tensorflow-datasets]\n",
      "   ---------------------------------- ----- 13/15 [tensorflow-datasets]\n",
      "   ---------------------------------- ----- 13/15 [tensorflow-datasets]\n",
      "   ---------------------------------- ----- 13/15 [tensorflow-datasets]\n",
      "   ---------------------------------- ----- 13/15 [tensorflow-datasets]\n",
      "   ---------------------------------- ----- 13/15 [tensorflow-datasets]\n",
      "   ---------------------------------- ----- 13/15 [tensorflow-datasets]\n",
      "   ---------------------------------- ----- 13/15 [tensorflow-datasets]\n",
      "   ---------------------------------- ----- 13/15 [tensorflow-datasets]\n",
      "   ---------------------------------- ----- 13/15 [tensorflow-datasets]\n",
      "   ---------------------------------- ----- 13/15 [tensorflow-datasets]\n",
      "   ---------------------------------- ----- 13/15 [tensorflow-datasets]\n",
      "   ---------------------------------- ----- 13/15 [tensorflow-datasets]\n",
      "   ---------------------------------- ----- 13/15 [tensorflow-datasets]\n",
      "   ---------------------------------- ----- 13/15 [tensorflow-datasets]\n",
      "   ---------------------------------- ----- 13/15 [tensorflow-datasets]\n",
      "   ---------------------------------- ----- 13/15 [tensorflow-datasets]\n",
      "   ------------------------------------- -- 14/15 [keras_cv]\n",
      "   ------------------------------------- -- 14/15 [keras_cv]\n",
      "   ------------------------------------- -- 14/15 [keras_cv]\n",
      "   ------------------------------------- -- 14/15 [keras_cv]\n",
      "   ------------------------------------- -- 14/15 [keras_cv]\n",
      "   ------------------------------------- -- 14/15 [keras_cv]\n",
      "   ------------------------------------- -- 14/15 [keras_cv]\n",
      "   ------------------------------------- -- 14/15 [keras_cv]\n",
      "   ---------------------------------------- 15/15 [keras_cv]\n",
      "\n",
      "Successfully installed dm-tree-0.1.9 docstring-parser-0.17.0 einops-0.8.1 etils-1.13.0 googleapis-common-protos-1.72.0 immutabledict-4.2.2 importlib_resources-6.5.2 kagglehub-0.4.0 kagglesdk-0.1.14 keras-core-0.1.7 keras_cv-0.9.0 promise-2.3 simple_parsing-0.1.7 tensorflow-datasets-4.9.9 tensorflow-metadata-1.17.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  DEPRECATION: Building 'promise' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'promise'. Discussion can be found at https://github.com/pypa/pip/issues/6334\n"
     ]
    }
   ],
   "source": [
    "!pip install keras_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df498508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting opencv-python\n",
      "  Downloading opencv_python-4.12.0.88-cp37-abi3-win_amd64.whl.metadata (19 kB)\n",
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.20.0-cp313-cp313-win_amd64.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: numpy<2.3.0,>=2 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from opencv-python) (2.1.3)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow)\n",
      "  Downloading absl_py-2.3.1-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow)\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=24.3.25 (from tensorflow)\n",
      "  Downloading flatbuffers-25.12.19-py2.py3-none-any.whl.metadata (1.0 kB)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow)\n",
      "  Downloading gast-0.7.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting google_pasta>=0.1.1 (from tensorflow)\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting libclang>=13.0.0 (from tensorflow)\n",
      "  Downloading libclang-18.1.1-py2.py3-none-win_amd64.whl.metadata (5.3 kB)\n",
      "Collecting opt_einsum>=2.3.2 (from tensorflow)\n",
      "  Downloading opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\lenovo\\appdata\\roaming\\python\\python313\\site-packages (from tensorflow) (25.0)\n",
      "Requirement already satisfied: protobuf>=5.28.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from tensorflow) (5.29.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from tensorflow) (72.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\lenovo\\appdata\\roaming\\python\\python313\\site-packages (from tensorflow) (1.17.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Downloading termcolor-3.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: typing_extensions>=3.6.6 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from tensorflow) (1.17.0)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow)\n",
      "  Downloading grpcio-1.76.0-cp313-cp313-win_amd64.whl.metadata (3.8 kB)\n",
      "Collecting tensorboard~=2.20.0 (from tensorflow)\n",
      "  Downloading tensorboard-2.20.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting keras>=3.10.0 (from tensorflow)\n",
      "  Downloading keras-3.13.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from tensorflow) (3.12.1)\n",
      "Collecting ml_dtypes<1.0.0,>=0.5.1 (from tensorflow)\n",
      "  Downloading ml_dtypes-0.5.4-cp313-cp313-win_amd64.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2025.4.26)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (3.8)\n",
      "Requirement already satisfied: pillow in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (11.1.0)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard~=2.20.0->tensorflow)\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: rich in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from keras>=3.10.0->tensorflow) (13.9.4)\n",
      "Collecting namex (from keras>=3.10.0->tensorflow)\n",
      "  Downloading namex-0.1.0-py3-none-any.whl.metadata (322 bytes)\n",
      "Collecting optree (from keras>=3.10.0->tensorflow)\n",
      "  Downloading optree-0.18.0-cp313-cp313-win_amd64.whl.metadata (35 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from rich->keras>=3.10.0->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\lenovo\\appdata\\roaming\\python\\python313\\site-packages (from rich->keras>=3.10.0->tensorflow) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow) (0.1.0)\n",
      "Downloading opencv_python-4.12.0.88-cp37-abi3-win_amd64.whl (39.0 MB)\n",
      "   ---------------------------------------- 0.0/39.0 MB ? eta -:--:--\n",
      "   --------- ------------------------------ 9.4/39.0 MB 54.8 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 22.5/39.0 MB 60.8 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 35.7/39.0 MB 62.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 39.0/39.0 MB 59.5 MB/s eta 0:00:00\n",
      "Downloading tensorflow-2.20.0-cp313-cp313-win_amd64.whl (332.0 MB)\n",
      "   ---------------------------------------- 0.0/332.0 MB ? eta -:--:--\n",
      "   - -------------------------------------- 14.7/332.0 MB 70.0 MB/s eta 0:00:05\n",
      "   --- ------------------------------------ 30.4/332.0 MB 73.1 MB/s eta 0:00:05\n",
      "   ----- ---------------------------------- 46.4/332.0 MB 73.4 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 60.3/332.0 MB 72.0 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 73.7/332.0 MB 70.7 MB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 84.9/332.0 MB 68.7 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 97.5/332.0 MB 67.4 MB/s eta 0:00:04\n",
      "   ------------- ------------------------- 112.2/332.0 MB 67.4 MB/s eta 0:00:04\n",
      "   -------------- ------------------------ 124.3/332.0 MB 66.8 MB/s eta 0:00:04\n",
      "   ---------------- ---------------------- 138.7/332.0 MB 66.7 MB/s eta 0:00:03\n",
      "   ----------------- --------------------- 153.1/332.0 MB 66.9 MB/s eta 0:00:03\n",
      "   ------------------- ------------------- 166.5/332.0 MB 66.6 MB/s eta 0:00:03\n",
      "   --------------------- ----------------- 179.6/332.0 MB 66.4 MB/s eta 0:00:03\n",
      "   ---------------------- ---------------- 195.0/332.0 MB 66.8 MB/s eta 0:00:03\n",
      "   ------------------------ -------------- 208.4/332.0 MB 66.7 MB/s eta 0:00:02\n",
      "   ------------------------- ------------- 220.5/332.0 MB 66.0 MB/s eta 0:00:02\n",
      "   --------------------------- ----------- 233.3/332.0 MB 65.6 MB/s eta 0:00:02\n",
      "   ---------------------------- ---------- 244.8/332.0 MB 65.0 MB/s eta 0:00:02\n",
      "   ----------------------------- --------- 252.2/332.0 MB 63.4 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 264.8/332.0 MB 62.8 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 276.6/332.0 MB 62.5 MB/s eta 0:00:01\n",
      "   --------------------------------- ----- 288.6/332.0 MB 61.7 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 300.4/332.0 MB 60.8 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 311.4/332.0 MB 60.3 MB/s eta 0:00:01\n",
      "   --------------------------------------  324.0/332.0 MB 59.8 MB/s eta 0:00:01\n",
      "   --------------------------------------- 332.0/332.0 MB 59.4 MB/s eta 0:00:00\n",
      "Downloading grpcio-1.76.0-cp313-cp313-win_amd64.whl (4.7 MB)\n",
      "   ---------------------------------------- 0.0/4.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 4.7/4.7 MB 68.9 MB/s eta 0:00:00\n",
      "Downloading ml_dtypes-0.5.4-cp313-cp313-win_amd64.whl (212 kB)\n",
      "Downloading tensorboard-2.20.0-py3-none-any.whl (5.5 MB)\n",
      "   ---------------------------------------- 0.0/5.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 5.5/5.5 MB 59.9 MB/s eta 0:00:00\n",
      "Downloading tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Downloading absl_py-2.3.1-py3-none-any.whl (135 kB)\n",
      "Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Downloading flatbuffers-25.12.19-py2.py3-none-any.whl (26 kB)\n",
      "Downloading gast-0.7.0-py3-none-any.whl (22 kB)\n",
      "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Downloading keras-3.13.0-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.5/1.5 MB 67.0 MB/s eta 0:00:00\n",
      "Downloading libclang-18.1.1-py2.py3-none-win_amd64.whl (26.4 MB)\n",
      "   ---------------------------------------- 0.0/26.4 MB ? eta -:--:--\n",
      "   ------------------- -------------------- 12.6/26.4 MB 61.8 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 24.4/26.4 MB 60.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 26.4/26.4 MB 60.3 MB/s eta 0:00:00\n",
      "Downloading opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "Downloading termcolor-3.3.0-py3-none-any.whl (7.7 kB)\n",
      "Downloading namex-0.1.0-py3-none-any.whl (5.9 kB)\n",
      "Downloading optree-0.18.0-cp313-cp313-win_amd64.whl (314 kB)\n",
      "Installing collected packages: namex, libclang, flatbuffers, termcolor, tensorboard-data-server, optree, opt_einsum, opencv-python, ml_dtypes, grpcio, google_pasta, gast, astunparse, absl-py, tensorboard, keras, tensorflow\n",
      "\n",
      "   -- -------------------------------------  1/17 [libclang]\n",
      "   -- -------------------------------------  1/17 [libclang]\n",
      "   ----------- ----------------------------  5/17 [optree]\n",
      "   -------------- -------------------------  6/17 [opt_einsum]\n",
      "   ---------------- -----------------------  7/17 [opencv-python]\n",
      "   ---------------- -----------------------  7/17 [opencv-python]\n",
      "   ------------------ ---------------------  8/17 [ml_dtypes]\n",
      "   --------------------- ------------------  9/17 [grpcio]\n",
      "   ----------------------- ---------------- 10/17 [google_pasta]\n",
      "   ---------------------------- ----------- 12/17 [astunparse]\n",
      "   -------------------------------- ------- 14/17 [tensorboard]\n",
      "   -------------------------------- ------- 14/17 [tensorboard]\n",
      "   -------------------------------- ------- 14/17 [tensorboard]\n",
      "   -------------------------------- ------- 14/17 [tensorboard]\n",
      "   -------------------------------- ------- 14/17 [tensorboard]\n",
      "   -------------------------------- ------- 14/17 [tensorboard]\n",
      "   -------------------------------- ------- 14/17 [tensorboard]\n",
      "   -------------------------------- ------- 14/17 [tensorboard]\n",
      "   ----------------------------------- ---- 15/17 [keras]\n",
      "   ----------------------------------- ---- 15/17 [keras]\n",
      "   ----------------------------------- ---- 15/17 [keras]\n",
      "   ----------------------------------- ---- 15/17 [keras]\n",
      "   ----------------------------------- ---- 15/17 [keras]\n",
      "   ----------------------------------- ---- 15/17 [keras]\n",
      "   ----------------------------------- ---- 15/17 [keras]\n",
      "   ----------------------------------- ---- 15/17 [keras]\n",
      "   ----------------------------------- ---- 15/17 [keras]\n",
      "   ----------------------------------- ---- 15/17 [keras]\n",
      "   ----------------------------------- ---- 15/17 [keras]\n",
      "   ----------------------------------- ---- 15/17 [keras]\n",
      "   ----------------------------------- ---- 15/17 [keras]\n",
      "   ----------------------------------- ---- 15/17 [keras]\n",
      "   ----------------------------------- ---- 15/17 [keras]\n",
      "   ----------------------------------- ---- 15/17 [keras]\n",
      "   ----------------------------------- ---- 15/17 [keras]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ------------------------------------- -- 16/17 [tensorflow]\n",
      "   ---------------------------------------- 17/17 [tensorflow]\n",
      "\n",
      "Successfully installed absl-py-2.3.1 astunparse-1.6.3 flatbuffers-25.12.19 gast-0.7.0 google_pasta-0.2.0 grpcio-1.76.0 keras-3.13.0 libclang-18.1.1 ml_dtypes-0.5.4 namex-0.1.0 opencv-python-4.12.0.88 opt_einsum-3.4.0 optree-0.18.0 tensorboard-2.20.0 tensorboard-data-server-0.7.2 tensorflow-2.20.0 termcolor-3.3.0\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python tensorflow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d83337e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting ultralytics\n",
      "  Downloading ultralytics-8.3.252-py3-none-any.whl.metadata (37 kB)\n",
      "Requirement already satisfied: numpy>=1.23.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from ultralytics) (2.1.3)\n",
      "Requirement already satisfied: matplotlib>=3.3.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from ultralytics) (3.10.0)\n",
      "Requirement already satisfied: opencv-python>=4.6.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from ultralytics) (4.12.0.88)\n",
      "Requirement already satisfied: pillow>=7.1.2 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from ultralytics) (11.1.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from ultralytics) (6.0.2)\n",
      "Requirement already satisfied: requests>=2.23.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from ultralytics) (2.32.3)\n",
      "Requirement already satisfied: scipy>=1.4.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from ultralytics) (1.15.3)\n",
      "Collecting torch>=1.8.0 (from ultralytics)\n",
      "  Downloading torch-2.9.1-cp313-cp313-win_amd64.whl.metadata (30 kB)\n",
      "Collecting torchvision>=0.9.0 (from ultralytics)\n",
      "  Downloading torchvision-0.24.1-cp313-cp313-win_amd64.whl.metadata (5.9 kB)\n",
      "Requirement already satisfied: psutil>=5.8.0 in c:\\users\\lenovo\\appdata\\roaming\\python\\python313\\site-packages (from ultralytics) (7.0.0)\n",
      "Collecting polars>=0.20.0 (from ultralytics)\n",
      "  Downloading polars-1.37.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting ultralytics-thop>=2.0.18 (from ultralytics)\n",
      "  Downloading ultralytics_thop-2.0.18-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\lenovo\\appdata\\roaming\\python\\python313\\site-packages (from matplotlib>=3.3.0->ultralytics) (25.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\lenovo\\appdata\\roaming\\python\\python313\\site-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
      "Collecting polars-runtime-32==1.37.0 (from polars>=0.20.0->ultralytics)\n",
      "  Downloading polars_runtime_32-1.37.0-cp310-abi3-win_amd64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\lenovo\\appdata\\roaming\\python\\python313\\site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests>=2.23.0->ultralytics) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests>=2.23.0->ultralytics) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests>=2.23.0->ultralytics) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests>=2.23.0->ultralytics) (2025.4.26)\n",
      "Requirement already satisfied: filelock in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from torch>=1.8.0->ultralytics) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from torch>=1.8.0->ultralytics) (1.13.3)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from torch>=1.8.0->ultralytics) (2025.3.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from torch>=1.8.0->ultralytics) (72.1.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch>=1.8.0->ultralytics) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n",
      "Downloading ultralytics-8.3.252-py3-none-any.whl (1.2 MB)\n",
      "   ---------------------------------------- 0.0/1.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.2/1.2 MB 29.1 MB/s eta 0:00:00\n",
      "Downloading polars-1.37.0-py3-none-any.whl (805 kB)\n",
      "   ---------------------------------------- 0.0/805.6 kB ? eta -:--:--\n",
      "   --------------------------------------- 805.6/805.6 kB 93.9 MB/s eta 0:00:00\n",
      "Downloading polars_runtime_32-1.37.0-cp310-abi3-win_amd64.whl (45.0 MB)\n",
      "   ---------------------------------------- 0.0/45.0 MB ? eta -:--:--\n",
      "   ---------- ----------------------------- 12.3/45.0 MB 58.8 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 25.7/45.0 MB 62.5 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 39.3/45.0 MB 64.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 45.0/45.0 MB 64.3 MB/s eta 0:00:00\n",
      "Downloading torch-2.9.1-cp313-cp313-win_amd64.whl (110.9 MB)\n",
      "   ---------------------------------------- 0.0/110.9 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 13.6/110.9 MB 67.3 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 27.0/110.9 MB 67.2 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 40.6/110.9 MB 65.3 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 54.3/110.9 MB 65.8 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 68.4/110.9 MB 65.9 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 81.5/110.9 MB 66.1 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 96.5/110.9 MB 66.4 MB/s eta 0:00:01\n",
      "   --------------------------------------- 110.9/110.9 MB 66.8 MB/s eta 0:00:00\n",
      "Downloading torchvision-0.24.1-cp313-cp313-win_amd64.whl (4.3 MB)\n",
      "   ---------------------------------------- 0.0/4.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 4.3/4.3 MB 61.6 MB/s eta 0:00:00\n",
      "Downloading ultralytics_thop-2.0.18-py3-none-any.whl (28 kB)\n",
      "Installing collected packages: polars-runtime-32, torch, polars, ultralytics-thop, torchvision, ultralytics\n",
      "\n",
      "   ---------------------------------------- 0/6 [polars-runtime-32]\n",
      "   ---------------------------------------- 0/6 [polars-runtime-32]\n",
      "   ---------------------------------------- 0/6 [polars-runtime-32]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------ --------------------------------- 1/6 [torch]\n",
      "   ------------- -------------------------- 2/6 [polars]\n",
      "   ------------- -------------------------- 2/6 [polars]\n",
      "   ------------- -------------------------- 2/6 [polars]\n",
      "   ------------- -------------------------- 2/6 [polars]\n",
      "   ------------- -------------------------- 2/6 [polars]\n",
      "   ------------- -------------------------- 2/6 [polars]\n",
      "   ------------- -------------------------- 2/6 [polars]\n",
      "   ------------- -------------------------- 2/6 [polars]\n",
      "   ------------- -------------------------- 2/6 [polars]\n",
      "   -------------------- ------------------- 3/6 [ultralytics-thop]\n",
      "   -------------------------- ------------- 4/6 [torchvision]\n",
      "   -------------------------- ------------- 4/6 [torchvision]\n",
      "   -------------------------- ------------- 4/6 [torchvision]\n",
      "   -------------------------- ------------- 4/6 [torchvision]\n",
      "   -------------------------- ------------- 4/6 [torchvision]\n",
      "   -------------------------- ------------- 4/6 [torchvision]\n",
      "   -------------------------- ------------- 4/6 [torchvision]\n",
      "   -------------------------- ------------- 4/6 [torchvision]\n",
      "   -------------------------- ------------- 4/6 [torchvision]\n",
      "   -------------------------- ------------- 4/6 [torchvision]\n",
      "   --------------------------------- ------ 5/6 [ultralytics]\n",
      "   --------------------------------- ------ 5/6 [ultralytics]\n",
      "   --------------------------------- ------ 5/6 [ultralytics]\n",
      "   --------------------------------- ------ 5/6 [ultralytics]\n",
      "   --------------------------------- ------ 5/6 [ultralytics]\n",
      "   --------------------------------- ------ 5/6 [ultralytics]\n",
      "   --------------------------------- ------ 5/6 [ultralytics]\n",
      "   --------------------------------- ------ 5/6 [ultralytics]\n",
      "   --------------------------------- ------ 5/6 [ultralytics]\n",
      "   --------------------------------- ------ 5/6 [ultralytics]\n",
      "   --------------------------------- ------ 5/6 [ultralytics]\n",
      "   --------------------------------- ------ 5/6 [ultralytics]\n",
      "   --------------------------------- ------ 5/6 [ultralytics]\n",
      "   --------------------------------- ------ 5/6 [ultralytics]\n",
      "   ---------------------------------------- 6/6 [ultralytics]\n",
      "\n",
      "Successfully installed polars-1.37.0 polars-runtime-32-1.37.0 torch-2.9.1 torchvision-0.24.1 ultralytics-8.3.252 ultralytics-thop-2.0.18\n"
     ]
    }
   ],
   "source": [
    "!pip install -U ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9578f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import keras_cv\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from scipy.spatial.distance import cosine\n",
    "import os\n",
    "from collections import deque, defaultdict\n",
    "import warnings\n",
    "from ultralytics import YOLO\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f050d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#  \n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "os.environ[\"OPENCV_VIDEOIO_PRIORITY_MSMF\"] = \"0\"\n",
    "cv2.setLogLevel(0)\n",
    "\n",
    "#   \n",
    "model = YOLO('C:// 3/yolo simlpe 8n.pt')\n",
    "\n",
    "#  EfficientNet   \n",
    "efficientnet = keras_cv.models.EfficientNetV2Backbone.from_preset(\"efficientnetv2_s\")\n",
    "print(\"EfficientNetV2Backbone \")\n",
    "\n",
    "#      YOLO\n",
    "class_names = model.names if hasattr(model, 'names') else {0: 'object'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0bbb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#    \n",
    "def preprocess_roi(roi):\n",
    "    \"\"\"Preprocess ROI for EfficientNet input\"\"\"\n",
    "    if roi is None or roi.size == 0:\n",
    "        return None\n",
    "    \n",
    "    #     \n",
    "    if roi.shape[0] <= 10 or roi.shape[1] <= 10:\n",
    "        return None\n",
    "        \n",
    "    #   ROI\n",
    "    if roi.shape[0] < 30 or roi.shape[1] < 30:\n",
    "        pad = 15\n",
    "        h, w = roi.shape[:2]\n",
    "        if h > 0 and w > 0:\n",
    "            padded = np.zeros((h + 2*pad, w + 2*pad, 3), dtype=roi.dtype)\n",
    "            padded[pad:pad+h, pad:pad+w] = roi\n",
    "            roi = padded\n",
    "    \n",
    "    roi = tf.image.resize(roi, (224, 224))\n",
    "    roi = tf.keras.applications.efficientnet.preprocess_input(roi)\n",
    "    return roi\n",
    "\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    \"\"\"Compute cosine similarity between two vectors with normalization\"\"\"\n",
    "    if np.linalg.norm(a) == 0 or np.linalg.norm(b) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    a = a / np.linalg.norm(a)\n",
    "    b = b / np.linalg.norm(b)\n",
    "    return np.dot(a, b)\n",
    "\n",
    "\n",
    "def calculate_iou(box1, box2):\n",
    "    \"\"\"Calculate IoU between two bounding boxes [x1, y1, x2, y2]\"\"\"\n",
    "    x1 = max(box1[0], box2[0])\n",
    "    y1 = max(box1[1], box2[1])\n",
    "    x2 = min(box1[2], box2[2])\n",
    "    y2 = min(box1[3], box2[3])\n",
    "    \n",
    "    intersection = max(0, x2 - x1) * max(0, y2 - y1)\n",
    "    area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "    area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "    union = area1 + area2 - intersection\n",
    "    \n",
    "    return intersection / union if union > 0 else 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53634af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_velocity_similarity(prev_box, curr_box, prev_velocity, frame_rate=30):\n",
    "    \"\"\"\n",
    "    Calculate similarity based on velocity consistency\n",
    "    Returns a score between 0 and 1 where 1 is perfect consistency\n",
    "    \"\"\"\n",
    "    # Calculate current velocity\n",
    "    prev_center = ((prev_box[0] + prev_box[2]) // 2, (prev_box[1] + prev_box[3]) // 2)\n",
    "    curr_center = ((curr_box[0] + curr_box[2]) // 2, (curr_box[1] + curr_box[3]) // 2)\n",
    "    \n",
    "    curr_velocity = (\n",
    "        (curr_center[0] - prev_center[0]) * frame_rate,\n",
    "        (curr_center[1] - prev_center[1]) * frame_rate\n",
    "    )\n",
    "    \n",
    "    if prev_velocity is None:\n",
    "        return 1.0  # No previous velocity to compare with\n",
    "    \n",
    "    # Calculate angle difference between velocities\n",
    "    dot_product = prev_velocity[0] * curr_velocity[0] + prev_velocity[1] * curr_velocity[1]\n",
    "    norm_prev = math.sqrt(prev_velocity[0]**2 + prev_velocity[1]**2)\n",
    "    norm_curr = math.sqrt(curr_velocity[0]**2 + curr_velocity[1]**2)\n",
    "    \n",
    "    if norm_prev == 0 or norm_curr == 0:\n",
    "        return 0.5  # Neutral score when one velocity is zero\n",
    "    \n",
    "    cos_theta = dot_product / (norm_prev * norm_curr + 1e-8)\n",
    "    angle_similarity = (cos_theta + 1) / 2  # Normalize to [0, 1]\n",
    "    \n",
    "    # Calculate magnitude similarity\n",
    "    magnitude_ratio = min(norm_prev, norm_curr) / max(norm_prev, norm_curr) if max(norm_prev, norm_curr) > 0 else 1.0\n",
    "    \n",
    "    # Combined velocity similarity\n",
    "    return 0.7 * angle_similarity + 0.3 * magnitude_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d392eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbOCTracker:\n",
    "    \n",
    "    def __init__(self, max_age=30, min_hits=3, iou_threshold=0.3, \n",
    "                 embedding_threshold=0.7, recovery_window=15, \n",
    "                 trajectory_length=50, frame_rate=30, \n",
    "                 velocity_weight=0.3, observation_confidence_alpha=0.7):\n",
    "        self.tracks = []  #   \n",
    "        self.next_id = 1  #  ID   \n",
    "        self.max_age = max_age  # .     \n",
    "        self.min_hits = min_hits  # .    \n",
    "        self.iou_threshold = iou_threshold\n",
    "        self.embedding_threshold = embedding_threshold\n",
    "        self.recovery_window = recovery_window  #    ID\n",
    "        self.trajectory_length = trajectory_length\n",
    "        self.lost_tracks = []  #   \n",
    "        self.frame_count = 0\n",
    "        self.frame_rate = frame_rate\n",
    "        self.trajectories = defaultdict(lambda: deque(maxlen=trajectory_length))\n",
    "        \n",
    "        # Observation-Centric \n",
    "        self.velocity_weight = velocity_weight  #     \n",
    "        self.observation_confidence_alpha = observation_confidence_alpha  #     \n",
    "        self.history_length = 10  #      \n",
    "\n",
    "    def estimate_observation_confidence(self, track, detection_box):\n",
    "        \"\"\"\n",
    "        Observation-Centric:      \n",
    "           0 ( )  1 ( )\n",
    "        \"\"\"\n",
    "        # 1.     \n",
    "        predicted_box = track.get('predicted_bbox', track['bbox'])\n",
    "        iou_consistency = calculate_iou(predicted_box, detection_box)\n",
    "        \n",
    "        # 2.      \n",
    "        trajectory = self.trajectories[track['track_id']]\n",
    "        if len(trajectory) > self.history_length:\n",
    "            #     \n",
    "            points = list(trajectory)[-self.history_length:]\n",
    "            linear_confidence = self._evaluate_motion_linearity(points)\n",
    "        else:\n",
    "            linear_confidence = 0.8  #     \n",
    "            \n",
    "        # 3.    \n",
    "        velocity_confidence = 1.0\n",
    "        if 'velocity_history' in track and len(track['velocity_history']) > 2:\n",
    "            velocity_confidence = self._evaluate_velocity_stability(track['velocity_history'])\n",
    "        \n",
    "        #      \n",
    "        observation_confidence = (\n",
    "            iou_consistency * 0.4 +\n",
    "            linear_confidence * 0.3 +\n",
    "            velocity_confidence * 0.3\n",
    "        )\n",
    "        \n",
    "        return max(0.1, min(1.0, observation_confidence))  #  \n",
    "\n",
    "    def _evaluate_motion_linearity(self, points):\n",
    "        \"\"\"\n",
    "             \n",
    "           0 ()  1 ()\n",
    "        \"\"\"\n",
    "        if len(points) < 3:\n",
    "            return 1.0\n",
    "        \n",
    "        #     \n",
    "        vectors = []\n",
    "        for i in range(1, len(points)):\n",
    "            dx = points[i][0] - points[i-1][0]\n",
    "            dy = points[i][1] - points[i-1][1]\n",
    "            vectors.append((dx, dy))\n",
    "        \n",
    "        #    \n",
    "        if all(abs(dx) < 2 and abs(dy) < 2 for dx, dy in vectors):\n",
    "            return 1.0\n",
    "        \n",
    "        #     \n",
    "        angle_consistency = []\n",
    "        for i in range(1, len(vectors)):\n",
    "            v1 = vectors[i-1]\n",
    "            v2 = vectors[i]\n",
    "            dot_product = v1[0]*v2[0] + v1[1]*v2[1]\n",
    "            norm1 = math.sqrt(v1[0]**2 + v1[1]**2)\n",
    "            norm2 = math.sqrt(v2[0]**2 + v2[1]**2)\n",
    "            \n",
    "            if norm1 > 0.1 and norm2 > 0.1:\n",
    "                cos_angle = dot_product / (norm1 * norm2)\n",
    "                angle_consistency.append(abs(cos_angle))\n",
    "        \n",
    "        if not angle_consistency:\n",
    "            return 0.8\n",
    "        \n",
    "        #   \n",
    "        avg_consistency = sum(angle_consistency) / len(angle_consistency)\n",
    "        \n",
    "        #    \n",
    "        if avg_consistency > 0.9:\n",
    "            return 1.0\n",
    "        #     ()\n",
    "        elif avg_consistency < 0.5:\n",
    "            return 0.3\n",
    "        #  \n",
    "        else:\n",
    "            return 0.6 + (avg_consistency - 0.5) * 0.8\n",
    "\n",
    "    def _evaluate_velocity_stability(self, velocity_history):\n",
    "        \"\"\"\n",
    "            \n",
    "           0 ()  1 ()\n",
    "        \"\"\"\n",
    "        if len(velocity_history) < 3:\n",
    "            return 1.0\n",
    "        \n",
    "        #   \n",
    "        accelerations = []\n",
    "        for i in range(1, len(velocity_history)):\n",
    "            prev_v = velocity_history[i-1]\n",
    "            curr_v = velocity_history[i]\n",
    "            acceleration = (\n",
    "                curr_v[0] - prev_v[0],\n",
    "                curr_v[1] - prev_v[1]\n",
    "            )\n",
    "            accelerations.append(math.sqrt(acceleration[0]**2 + acceleration[1]**2))\n",
    "        \n",
    "        if not accelerations:\n",
    "            return 1.0\n",
    "        \n",
    "        #     \n",
    "        mean_accel = sum(accelerations) / len(accelerations)\n",
    "        if mean_accel < 1.0:  #  \n",
    "            return 1.0\n",
    "        \n",
    "        #   (  )\n",
    "        if len(accelerations) > 1:\n",
    "            std_accel = np.std(accelerations)\n",
    "            cv = std_accel / mean_accel if mean_accel > 0 else 1.0\n",
    "            stability = max(0.2, 1.0 - cv)\n",
    "        else:\n",
    "            stability = 0.8\n",
    "        \n",
    "        return stability\n",
    "\n",
    "    def observation_centric_reupdate(self, track, detection, observation_confidence):\n",
    "        \"\"\"\n",
    "        Observation-Centric Re-Update (ORU) \n",
    "               \n",
    "        \"\"\"\n",
    "        #   \n",
    "        current_bbox = track['bbox']\n",
    "        detection_box = detection[0]\n",
    "        \n",
    "        #      \n",
    "        #    -   \n",
    "        #    -   \n",
    "        update_weight = min(1.0, observation_confidence * 1.5)\n",
    "        \n",
    "        #   bbox\n",
    "        new_bbox = [\n",
    "            current_bbox[0] * (1 - update_weight) + detection_box[0] * update_weight,\n",
    "            current_bbox[1] * (1 - update_weight) + detection_box[1] * update_weight,\n",
    "            current_bbox[2] * (1 - update_weight) + detection_box[2] * update_weight,\n",
    "            current_bbox[3] * (1 - update_weight) + detection_box[3] * update_weight\n",
    "        ]\n",
    "        \n",
    "        #     \n",
    "        current_embedding = track['embedding']\n",
    "        detection_embedding = detection[1]\n",
    "        embedding_weight = min(1.0, observation_confidence * 0.8)  #     \n",
    "        \n",
    "        new_embedding = (\n",
    "            current_embedding * (1 - embedding_weight) + \n",
    "            detection_embedding * embedding_weight\n",
    "        )\n",
    "        \n",
    "        #  \n",
    "        center = ((new_bbox[0] + new_bbox[2]) // 2, (new_bbox[1] + new_bbox[3]) // 2)\n",
    "        prev_center = ((current_bbox[0] + current_bbox[2]) // 2, (current_bbox[1] + current_bbox[3]) // 2)\n",
    "        \n",
    "        if 'velocity' not in track:\n",
    "            track['velocity'] = (0, 0)\n",
    "            track['velocity_history'] = deque(maxlen=10)\n",
    "        \n",
    "        #   \n",
    "        velocity = (\n",
    "            (center[0] - prev_center[0]) * self.frame_rate,\n",
    "            (center[1] - prev_center[1]) * self.frame_rate\n",
    "        )\n",
    "        \n",
    "        #  \n",
    "        velocity_smoothing = 0.7 if observation_confidence > 0.7 else 0.4\n",
    "        smoothed_velocity = (\n",
    "            track['velocity'][0] * (1 - velocity_smoothing) + velocity[0] * velocity_smoothing,\n",
    "            track['velocity'][1] * (1 - velocity_smoothing) + velocity[1] * velocity_smoothing\n",
    "        )\n",
    "        \n",
    "        #   \n",
    "        track['bbox'] = new_bbox\n",
    "        track['embedding'] = new_embedding\n",
    "        track['velocity'] = smoothed_velocity\n",
    "        track['velocity_history'].append(velocity)\n",
    "        \n",
    "        #      \n",
    "        track['predicted_bbox'] = self._predict_next_bbox(new_bbox, smoothed_velocity)\n",
    "        \n",
    "        return track\n",
    "\n",
    "    def _predict_next_bbox(self, current_bbox, velocity):\n",
    "        \"\"\"\n",
    "           bbox   \n",
    "        \"\"\"\n",
    "        center = ((current_bbox[0] + current_bbox[2]) // 2, (current_bbox[1] + current_bbox[3]) // 2)\n",
    "        width = current_bbox[2] - current_bbox[0]\n",
    "        height = current_bbox[3] - current_bbox[1]\n",
    "        \n",
    "        #    \n",
    "        next_center = (\n",
    "            center[0] + velocity[0] / self.frame_rate,\n",
    "            center[1] + velocity[1] / self.frame_rate\n",
    "        )\n",
    "        \n",
    "        #  bbox\n",
    "        next_bbox = [\n",
    "            next_center[0] - width / 2,\n",
    "            next_center[1] - height / 2,\n",
    "            next_center[0] + width / 2,\n",
    "            next_center[1] + height / 2\n",
    "        ]\n",
    "        \n",
    "        return next_bbox\n",
    "\n",
    "    def update(self, detections):\n",
    "        self.frame_count += 1\n",
    "        \n",
    "        #   ,    \n",
    "        if not detections:\n",
    "            for track in self.tracks:\n",
    "                track['time_since_update'] += 1\n",
    "                track['age'] += 1\n",
    "                \n",
    "                #   \n",
    "                if 'velocity' in track:\n",
    "                    track['bbox'] = self._predict_next_bbox(\n",
    "                        track['bbox'], \n",
    "                        track['velocity']\n",
    "                    )\n",
    "            \n",
    "            #   \n",
    "            self.tracks = [t for t in self.tracks \n",
    "                          if t['time_since_update'] <= self.max_age and t['hits'] >= 1]\n",
    "            return [(t['track_id'], t['bbox'], t['class_id']) for t in self.tracks \n",
    "                   if t['hits'] >= 1 and t['time_since_update'] == 0]\n",
    "        \n",
    "        current_boxes = [det[0] for det in detections]\n",
    "        current_embeddings = [det[1] for det in detections]\n",
    "        current_classes = [det[2] for det in detections]\n",
    "        \n",
    "        #     \n",
    "        all_tracks = self.tracks.copy()\n",
    "        cost_matrix = np.zeros((len(all_tracks), len(current_boxes)))\n",
    "        \n",
    "        #      Observation-Centric \n",
    "        for i, track in enumerate(all_tracks):\n",
    "            for j, (box, emb) in enumerate(zip(current_boxes, current_embeddings)):\n",
    "                #  \n",
    "                emb_sim = cosine_similarity(track['embedding'], emb)\n",
    "                iou = calculate_iou(track['bbox'], box)\n",
    "                \n",
    "                #    Observation-Centric \n",
    "                velocity_sim = 1.0\n",
    "                if 'velocity' in track:\n",
    "                    velocity_sim = calculate_velocity_similarity(\n",
    "                        track['bbox'], box, track['velocity'], self.frame_rate\n",
    "                    )\n",
    "                \n",
    "                #   \n",
    "                obs_confidence = self.estimate_observation_confidence(track, box)\n",
    "                \n",
    "                #     \n",
    "                #     -    \n",
    "                #    -    \n",
    "                if obs_confidence < 0.4:\n",
    "                    combined_score = (0.8 * emb_sim + 0.2 * iou)\n",
    "                else:\n",
    "                    combined_score = (\n",
    "                        0.5 * emb_sim + \n",
    "                        0.3 * iou + \n",
    "                        0.2 * velocity_sim\n",
    "                    )\n",
    "                \n",
    "                #    \n",
    "                if track['class_id'] != current_classes[j]:\n",
    "                    combined_score *= 0.5\n",
    "                \n",
    "                #      \n",
    "                combined_score *= obs_confidence\n",
    "                \n",
    "                cost_matrix[i, j] = 1.0 - combined_score if combined_score > 0.2 else 10.0\n",
    "        \n",
    "        #    \n",
    "        matched_tracks = set()\n",
    "        matched_detections = set()\n",
    "        \n",
    "        if cost_matrix.size > 0 and len(all_tracks) > 0 and len(current_boxes) > 0:\n",
    "            row_ind, col_ind = linear_sum_assignment(cost_matrix)\n",
    "            \n",
    "            for i, j in zip(row_ind, col_ind):\n",
    "                if i < len(all_tracks) and j < len(current_boxes):\n",
    "                    cost = cost_matrix[i, j]\n",
    "                    track = all_tracks[i]\n",
    "                    detection = (current_boxes[j], current_embeddings[j], current_classes[j])\n",
    "                    \n",
    "                    #       \n",
    "                    obs_confidence = self.estimate_observation_confidence(track, current_boxes[j])\n",
    "                    dynamic_threshold = 1.0 - (self.embedding_threshold * obs_confidence * 1.2)\n",
    "                    \n",
    "                    if cost < dynamic_threshold:\n",
    "                        # Observation-Centric Re-Update\n",
    "                        track = self.observation_centric_reupdate(track, detection, obs_confidence)\n",
    "                        \n",
    "                        #  \n",
    "                        track['hits'] += 1\n",
    "                        track['age'] += 1\n",
    "                        track['time_since_update'] = 0\n",
    "                        track['class_id'] = current_classes[j]\n",
    "                        \n",
    "                        #  \n",
    "                        center = (int((track['bbox'][0] + track['bbox'][2]) // 2),\n",
    "                                 int((track['bbox'][1] + track['bbox'][3]) // 2))\n",
    "                        self.trajectories[track['track_id']].append(center)\n",
    "                        \n",
    "                        matched_tracks.add(i)\n",
    "                        matched_detections.add(j)\n",
    "        \n",
    "        #   \n",
    "        for i, track in enumerate(all_tracks):\n",
    "            if i not in matched_tracks:\n",
    "                track['time_since_update'] += 1\n",
    "                track['age'] += 1\n",
    "                \n",
    "                #      \n",
    "                if 'velocity' in track:\n",
    "                    track['bbox'] = self._predict_next_bbox(\n",
    "                        track['bbox'], \n",
    "                        track['velocity']\n",
    "                    )\n",
    "                \n",
    "                #   lost_tracks   \n",
    "                if track['time_since_update'] > self.max_age // 2:\n",
    "                    self.lost_tracks.append({\n",
    "                        'track_id': track['track_id'],\n",
    "                        'bbox': track['bbox'],\n",
    "                        'embedding': track['embedding'],\n",
    "                        'class_id': track['class_id'],\n",
    "                        'age': track['age'],\n",
    "                        'lost_frame': self.frame_count,\n",
    "                        'velocity': track.get('velocity', (0, 0)),\n",
    "                        'trajectory': list(self.trajectories[track['track_id']])\n",
    "                    })\n",
    "        \n",
    "        #      \n",
    "        for j, (box, emb, cls_id) in enumerate(zip(current_boxes, current_embeddings, current_classes)):\n",
    "            if j not in matched_detections:\n",
    "                #       \n",
    "                initial_velocity = self._estimate_initial_velocity(box)\n",
    "                \n",
    "                track_id = self.next_id\n",
    "                self.next_id += 1\n",
    "                \n",
    "                new_track = {\n",
    "                    'track_id': track_id,\n",
    "                    'bbox': box,\n",
    "                    'embedding': emb,\n",
    "                    'class_id': cls_id,\n",
    "                    'hits': 1,\n",
    "                    'age': 1,\n",
    "                    'time_since_update': 0,\n",
    "                    'velocity': initial_velocity,\n",
    "                    'velocity_history': deque(maxlen=10),\n",
    "                }\n",
    "                \n",
    "                #   \n",
    "                new_track['predicted_bbox'] = self._predict_next_bbox(box, initial_velocity)\n",
    "                \n",
    "                self.tracks.append(new_track)\n",
    "                \n",
    "                #  \n",
    "                center = ((box[0] + box[2]) // 2, (box[1] + box[3]) // 2)\n",
    "                self.trajectories[track_id].append(center)\n",
    "        \n",
    "        #    lost_tracks\n",
    "        self._recover_tracks(detections)\n",
    "        \n",
    "        #   \n",
    "        self.tracks = [t for t in self.tracks \n",
    "                      if t['time_since_update'] <= self.max_age and t['hits'] >= 1]\n",
    "        \n",
    "        #   lost_tracks\n",
    "        self.lost_tracks = [t for t in self.lost_tracks \n",
    "                           if self.frame_count - t['lost_frame'] <= self.recovery_window]\n",
    "        \n",
    "        #   \n",
    "        return [(t['track_id'], t['bbox'], t['class_id']) for t in self.tracks \n",
    "               if t['hits'] >= self.min_hits and t['time_since_update'] == 0]\n",
    "\n",
    "    def _estimate_initial_velocity(self, bbox):\n",
    "        \"\"\"\n",
    "                 \n",
    "        \"\"\"\n",
    "        if not self.tracks:\n",
    "            return (0, 0)\n",
    "        \n",
    "        #   \n",
    "        nearby_tracks = []\n",
    "        center = ((bbox[0] + bbox[2]) // 2, (bbox[1] + bbox[3]) // 2)\n",
    "        \n",
    "        for track in self.tracks:\n",
    "            if track['time_since_update'] == 0:  #   \n",
    "                track_center = ((track['bbox'][0] + track['bbox'][2]) // 2, \n",
    "                               (track['bbox'][1] + track['bbox'][3]) // 2)\n",
    "                distance = math.sqrt(\n",
    "                    (center[0] - track_center[0])**2 + \n",
    "                    (center[1] - track_center[1])**2\n",
    "                )\n",
    "                if distance < 100:  #    \"\" \n",
    "                    nearby_tracks.append(track)\n",
    "        \n",
    "        if not nearby_tracks:\n",
    "            return (0, 0)\n",
    "        \n",
    "        #    \n",
    "        total_weight = 0\n",
    "        velocity_sum = [0, 0]\n",
    "        \n",
    "        for track in nearby_tracks:\n",
    "            distance = math.sqrt(\n",
    "                (center[0] - ((track['bbox'][0] + track['bbox'][2]) // 2))**2 + \n",
    "                (center[1] - ((track['bbox'][1] + track['bbox'][3]) // 2))**2\n",
    "            )\n",
    "            weight = 1.0 / max(1.0, distance)  #     \n",
    "            \n",
    "            total_weight += weight\n",
    "            velocity_sum[0] += track['velocity'][0] * weight\n",
    "            velocity_sum[1] += track['velocity'][1] * weight\n",
    "        \n",
    "        if total_weight > 0:\n",
    "            avg_velocity = (\n",
    "                velocity_sum[0] / total_weight,\n",
    "                velocity_sum[1] / total_weight\n",
    "            )\n",
    "            return avg_velocity\n",
    "        \n",
    "        return (0, 0)\n",
    "\n",
    "    def _recover_tracks(self, detections):\n",
    "        \"\"\"\n",
    "           lost_tracks    \n",
    "        \"\"\"\n",
    "        if not self.lost_tracks or not detections:\n",
    "            return\n",
    "        \n",
    "        current_boxes = [det[0] for det in detections]\n",
    "        current_embeddings = [det[1] for det in detections]\n",
    "        current_classes = [det[2] for det in detections]\n",
    "        \n",
    "        #    lost_tracks  \n",
    "        recovery_cost = np.zeros((len(self.lost_tracks), len(current_boxes)))\n",
    "        \n",
    "        for i, lost_track in enumerate(self.lost_tracks):\n",
    "            for j, (box, emb) in enumerate(zip(current_boxes, current_embeddings)):\n",
    "                #  \n",
    "                emb_sim = cosine_similarity(lost_track['embedding'], emb)\n",
    "                iou = calculate_iou(lost_track['bbox'], box)\n",
    "                \n",
    "                #  \n",
    "                velocity_sim = 1.0\n",
    "                if 'velocity' in lost_track:\n",
    "                    velocity_sim = calculate_velocity_similarity(\n",
    "                        lost_track['bbox'], box, lost_track['velocity'], self.frame_rate\n",
    "                    )\n",
    "                \n",
    "                #    \n",
    "                recovery_score = (\n",
    "                    0.5 * emb_sim + \n",
    "                    0.3 * iou + \n",
    "                    0.2 * velocity_sim\n",
    "                )\n",
    "                \n",
    "                #     \n",
    "                time_penalty = max(0.5, 1.0 - (self.frame_count - lost_track['lost_frame']) / self.recovery_window)\n",
    "                recovery_score *= time_penalty\n",
    "                \n",
    "                #    \n",
    "                if lost_track['class_id'] != current_classes[j]:\n",
    "                    recovery_score *= 0.3\n",
    "                \n",
    "                recovery_cost[i, j] = 1.0 - recovery_score if recovery_score > 0.4 else 10.0\n",
    "        \n",
    "        #    \n",
    "        recovered_indices = set()\n",
    "        used_detections = set()\n",
    "        \n",
    "        if recovery_cost.size > 0:\n",
    "            row_ind, col_ind = linear_sum_assignment(recovery_cost)\n",
    "            \n",
    "            for i, j in zip(row_ind, col_ind):\n",
    "                if i < len(self.lost_tracks) and j < len(current_boxes):\n",
    "                    cost = recovery_cost[i, j]\n",
    "                    \n",
    "                    #       \n",
    "                    lost_track = self.lost_tracks[i]\n",
    "                    time_absent = self.frame_count - lost_track['lost_frame']\n",
    "                    recovery_threshold = 0.6 * (1.0 - time_absent / self.recovery_window)\n",
    "                    \n",
    "                    if cost < recovery_threshold and j not in used_detections:\n",
    "                        #  \n",
    "                        track_id = lost_track['track_id']\n",
    "                        new_track = {\n",
    "                            'track_id': track_id,\n",
    "                            'bbox': current_boxes[j],\n",
    "                            'embedding': current_embeddings[j],\n",
    "                            'class_id': current_classes[j],\n",
    "                            'hits': 1,\n",
    "                            'age': lost_track['age'] + 1,\n",
    "                            'time_since_update': 0,\n",
    "                            'velocity': lost_track.get('velocity', (0, 0)),\n",
    "                            'velocity_history': deque(maxlen=10),\n",
    "                        }\n",
    "                        \n",
    "                        #   \n",
    "                        if 'velocity' in lost_track:\n",
    "                            new_track['velocity_history'].append(lost_track['velocity'])\n",
    "                        \n",
    "                        #  \n",
    "                        if track_id not in self.trajectories:\n",
    "                            self.trajectories[track_id] = deque(maxlen=self.trajectory_length)\n",
    "                            for point in lost_track['trajectory']:\n",
    "                                self.trajectories[track_id].append(point)\n",
    "                        \n",
    "                        self.tracks.append(new_track)\n",
    "                        \n",
    "                        recovered_indices.add(i)\n",
    "                        used_detections.add(j)\n",
    "        \n",
    "        #     lost_tracks\n",
    "        self.lost_tracks = [t for i, t in enumerate(self.lost_tracks) if i not in recovered_indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774a4492",
   "metadata": {},
   "outputs": [],
   "source": [
    "def track_objects(video_path, output_path=None):\n",
    "    #   \n",
    "    if not os.path.exists(video_path):\n",
    "        print(f\":    : {video_path}\")\n",
    "        return\n",
    "    \n",
    "    #  \n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(\":    \")\n",
    "        return\n",
    "    \n",
    "\n",
    "    #    Observation-Centric \n",
    "    tracker = EmbOCTracker(\n",
    "        max_age=65,\n",
    "        min_hits=2,\n",
    "        iou_threshold=0.15,\n",
    "        embedding_threshold=0.35,\n",
    "        recovery_window=25,\n",
    "        frame_rate=40,  \n",
    "        velocity_weight=0.25,\n",
    "        observation_confidence_alpha=0.65\n",
    "    )\n",
    "    \n",
    "    #  \n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    \n",
    "    #  \n",
    "    output_writer = None\n",
    "    if output_path:\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        output_writer = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "    \n",
    "    print(f\"  : {video_path}\")\n",
    "    print(f\" : {width}x{height}, FPS: {fps}\")\n",
    "    \n",
    "    frame_count = 0\n",
    "    total_objects = 0\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        #  \n",
    "        results = model.predict(frame, conf=0.3, verbose=False)\n",
    "        \n",
    "        #  \n",
    "        detections = []\n",
    "        for result in results:\n",
    "            if not hasattr(result, 'boxes') or result.boxes is None or len(result.boxes) == 0:\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                boxes = result.boxes.xyxy.cpu().numpy()\n",
    "                classes = result.boxes.cls.cpu().numpy()\n",
    "                \n",
    "                for i in range(len(boxes)):\n",
    "                    x1, y1, x2, y2 = map(int, boxes[i])\n",
    "                    cls_id = int(classes[i])\n",
    "                    \n",
    "                    #   \n",
    "                    if x2 <= x1 or y2 <= y1:\n",
    "                        continue\n",
    "                    \n",
    "                    #  ROI   \n",
    "                    roi = frame[y1:y2, x1:x2]\n",
    "                    if roi.size == 0:\n",
    "                        continue\n",
    "                    \n",
    "                    processed_roi = preprocess_roi(roi)\n",
    "                    if processed_roi is None:\n",
    "                        continue\n",
    "                    \n",
    "                    try:\n",
    "                        embedding = efficientnet.predict(processed_roi[np.newaxis, ...], verbose=0).flatten()\n",
    "                        detections.append(((x1, y1, x2, y2), embedding, cls_id))\n",
    "                    except Exception as e:\n",
    "                        print(f\"  : {e}\")\n",
    "                        continue\n",
    "            except Exception as e:\n",
    "                print(f\"   : {e}\")\n",
    "                continue\n",
    "        \n",
    "        #     Observation-Centric \n",
    "        tracks = tracker.update(detections)\n",
    "        \n",
    "        #     \n",
    "        if tracks:\n",
    "            total_objects = max(total_objects, max([t[0] for t in tracks]))\n",
    "        \n",
    "        #  \n",
    "        for track_id, bbox, class_id in tracks:\n",
    "            x1, y1, x2, y2 = map(int, bbox)\n",
    "            class_name = class_names.get(class_id, f'class{class_id}')\n",
    "            \n",
    "            #     ID\n",
    "            color = (int(track_id * 30 % 255), \n",
    "                    int((track_id * 50 + 50) % 255), \n",
    "                    int((track_id * 70 + 100) % 255))\n",
    "            \n",
    "            #  bounding box\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
    "            \n",
    "            #  :   + ID  +  \n",
    "            if track_id in tracker.trajectories:\n",
    "                track = next((t for t in tracker.tracks if t['track_id'] == track_id), None)\n",
    "                if track:\n",
    "                    obs_confidence = tracker.estimate_observation_confidence(track, bbox) if 'predicted_bbox' in track else 1.0\n",
    "                    label = f\"{class_name} ID:{track_id} CF:{obs_confidence:.2f}\"\n",
    "                else:\n",
    "                    label = f\"{class_name} ID:{track_id}\"\n",
    "                \n",
    "                cv2.putText(frame, label, (x1, y1 - 10),\n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)\n",
    "            \n",
    "            #  \n",
    "            if track_id in tracker.trajectories and len(tracker.trajectories[track_id]) > 1:\n",
    "                points = list(tracker.trajectories[track_id])\n",
    "                for j in range(1, len(points)):\n",
    "                    thickness = max(1, 3 - j // 10)  #  \n",
    "                    cv2.line(frame, points[j-1], points[j], color, thickness)\n",
    "        \n",
    "        #     \n",
    "        for track in tracker.tracks:\n",
    "            if 'velocity_history' in track and len(track['velocity_history']) > 5:\n",
    "                #   \n",
    "                linearity = tracker._evaluate_motion_linearity(list(tracker.trajectories[track['track_id']]))\n",
    "                if linearity < 0.6:  \n",
    "                    cx = int(round((track['bbox'][0] + track['bbox'][2]) / 2))\n",
    "                    cy = int(round((track['bbox'][1] + track['bbox'][3]) / 2))\n",
    "                    cv2.circle(frame, (cx, cy), 8, (0, 0, 255), 2)\n",
    "        \n",
    "        #   \n",
    "        cv2.putText(frame, f\"Frame: {frame_count}/{int(cap.get(cv2.CAP_PROP_FRAME_COUNT))}\", (10, 30), \n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "        cv2.putText(frame, f\"Det: {len(detections)} | Trk: {len(tracks)} | Tot: {total_objects}\", (10, 60), \n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "        \n",
    "        #   \n",
    "        if output_writer:\n",
    "            output_writer.write(frame)\n",
    "        \n",
    "        cv2.imshow('Tracking', frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "        \n",
    "       \n",
    "    #  \n",
    "    cap.release()\n",
    "    if output_writer:\n",
    "        output_writer.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07934c0",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d18529d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EfficientNetV2Backbone \n",
      "  : C:// 3/data/TestVideo/UAV and hands 2.mp4\n",
      " : 720x1280, FPS: 30.0\n"
     ]
    }
   ],
   "source": [
    "video_path = \"C:// 3/data/TestVideo/UAV and hands 2.mp4\"\n",
    "output_path = \"UAV and hands full yolo 2.mp4\"\n",
    "    \n",
    "track_objects(video_path, output_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
